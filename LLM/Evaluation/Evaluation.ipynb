{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4388b4-93d5-4f49-85c4-d9c4964eb50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "langsmith_api_key = os.getenv('LANGSMITH_API_KEY')\n",
    "huggingface_api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "os.environ['LANGSMITH_TRACING'] = 'true'\n",
    "os.environ['LANGSMITH_API_KEY'] = langsmith_api_key\n",
    "os.environ['HUGGINGFACE_API_KEY'] = huggingface_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0cd38e6-83c7-45a3-a463-344532873e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader # Use this\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d4d52f-3d9c-43f4-b466-eecd53b0d0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and Processing PDFs...\n",
      "-> Loading: BES-COVID-Pract-Recomnd-06-June-Final-Copy.pdf\n",
      "   Loaded 38 pages.\n",
      "-> Loading: BES-Ramadan-Guideline-2020-min.pdf\n",
      "   Loaded 46 pages.\n",
      "-> Loading: Diabetes_Care_BADAS_guideline2019-3.pdf\n",
      "   Loaded 79 pages.\n",
      "-> Loading: Insulin-Guideline-min.pdf\n",
      "   Loaded 93 pages.\n",
      "\n",
      "Total documents loaded: 256\n",
      "\n",
      "Sample Document Metadata (first doc):\n",
      "{'producer': 'Nitro PDF PrimoPDF', 'creator': 'PrimoPDF http://www.primopdf.com', 'creationdate': '2020-06-07T20:17:39-06:00', 'moddate': '2020-06-07T20:17:39-06:00', 'title': 'Microsoft Word - BES COVID Pract Recomnd 06 June Final Copy', 'author': 'Mir', 'source': 'BES-COVID-Pract-Recomnd-06-June-Final-Copy.pdf', 'total_pages': 38, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Sample Document Content (first 500 chars of first doc):\n",
      "Bangladesh Endocrine Society (BES) \n",
      "Practical Recommendations for Management of \n",
      "Diabetes and Other Endocrine Diseases in Patients with \n",
      "COVID-19 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Published Online June 2020 \n",
      " \n",
      " \n",
      "All rights reserved by: Bangladesh Endocrine Society (BES) \n",
      " \n",
      " \n",
      "Published by \n",
      "Bangladesh Endocrine Society (BES) \n",
      "Website: http://bes-org.net \n",
      "E-mail: \n",
      "endobd2012@gmail.com\n"
     ]
    }
   ],
   "source": [
    "pdf_files = [\n",
    "    r\"E:\\DiabetIQ\\LLM\\PDFs\\BES-COVID-Pract-Recomnd-06-June-Final-Copy.pdf\",\n",
    "    r\"E:\\DiabetIQ\\LLM\\PDFs\\BES-Ramadan-Guideline-2020-min.pdf\",\n",
    "    r\"E:\\DiabetIQ\\LLM\\PDFs\\Diabetes_Care_BADAS_guideline2019-3.pdf\",\n",
    "    r\"E:\\DiabetIQ\\LLM\\PDFs\\Insulin-Guideline-min.pdf\"\n",
    "]\n",
    "\n",
    "all_docs = [] # Will store LangChain Document objects\n",
    "\n",
    "print(\"Loading and Processing PDFs...\")\n",
    "for pdf_path in pdf_files:\n",
    "    try:\n",
    "        # Extract filename for metadata\n",
    "        file_name = os.path.basename(pdf_path)\n",
    "        print(f\"-> Loading: {file_name}\")\n",
    "\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        # Load pages as individual documents. Each doc will have metadata['page']\n",
    "        pages = loader.load_and_split() # This does basic splitting\n",
    "\n",
    "        # Add source filename to metadata for each page/document\n",
    "        for page_doc in pages:\n",
    "            page_doc.metadata['source'] = file_name\n",
    "            # Optional: clean up page content slightly if needed\n",
    "            # page_doc.page_content = page_doc.page_content.replace('\\n', ' ').strip()\n",
    "\n",
    "        all_docs.extend(pages)\n",
    "        print(f\"   Loaded {len(pages)} pages.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {pdf_path}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(all_docs)}\")\n",
    "if all_docs:\n",
    "    print(\"\\nSample Document Metadata (first doc):\")\n",
    "    print(all_docs[0].metadata)\n",
    "    print(\"\\nSample Document Content (first 500 chars of first doc):\")\n",
    "    print(all_docs[0].page_content[:500])\n",
    "else:\n",
    "    print(\"\\nNo documents were loaded successfully.\")\n",
    "    # Consider exiting or handling this error appropriately\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "411eec36-c71e-47b1-bcf2-6a247991c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    # Keep separators that make sense for text structure\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"],\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5bc65c4-d068-46d2-8f90-23470940c849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total chunks created: 702\n",
      "\n",
      "Sample Chunk Metadata (first chunk):\n",
      "{'producer': 'Nitro PDF PrimoPDF', 'creator': 'PrimoPDF http://www.primopdf.com', 'creationdate': '2020-06-07T20:17:39-06:00', 'moddate': '2020-06-07T20:17:39-06:00', 'title': 'Microsoft Word - BES COVID Pract Recomnd 06 June Final Copy', 'author': 'Mir', 'source': 'BES-COVID-Pract-Recomnd-06-June-Final-Copy.pdf', 'total_pages': 38, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Sample Chunk Content (first 500 chars):\n",
      "Bangladesh Endocrine Society (BES) \n",
      "Practical Recommendations for Management of \n",
      "Diabetes and Other Endocrine Diseases in Patients with \n",
      "COVID-19 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Published Online June 2020 \n",
      " \n",
      " \n",
      "All rights reserved by: Bangladesh Endocrine Society (BES) \n",
      " \n",
      " \n",
      "Published by \n",
      "Bangladesh Endocrine Society (BES) \n",
      "Website: http://bes-org.net \n",
      "E-mail: \n",
      "endobd2012@gmail.com\n"
     ]
    }
   ],
   "source": [
    "chunks = text_splitter.split_documents(all_docs)\n",
    "\n",
    "print(f\"\\nTotal chunks created: {len(chunks)}\")\n",
    "if chunks:\n",
    "    print(\"\\nSample Chunk Metadata (first chunk):\")\n",
    "    print(chunks[0].metadata)\n",
    "    print(\"\\nSample Chunk Content (first 500 chars):\")\n",
    "    print(chunks[0].page_content[:500])\n",
    "else:\n",
    "    print(\"\\nNo chunks were created. Check splitting process.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "469c5497-9fc3-4614-a72e-b5729c4f6156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Embedding Model...\n",
      "\n",
      "Creating Vector Store (ChromaDB)...\n",
      "Vector Store Created.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInitializing Embedding Model...\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/e5-small-v2\")\n",
    "\n",
    "print(\"\\nCreating Vector Store (ChromaDB)...\")\n",
    "# Chroma.from_documents handles Document objects directly\n",
    "# Consider adding persistence: persist_directory=\"./chroma_db_diabetiq\"\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    # persist_directory=\"./chroma_db_diabetiq\" # Uncomment to save DB locally\n",
    ")\n",
    "# If persisting: vectorstore.persist()\n",
    "\n",
    "# To load later:\n",
    "# vectorstore = Chroma(persist_directory=\"./chroma_db_diabetiq\", embedding_function=embedding_model)\n",
    "\n",
    "print(\"Vector Store Created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8099d3b3-1b42-4b0c-8881-ba377501da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5}) # Retrieve top 5 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ece2fd5-8f00-45b0-b4ef-afe7f1d107bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "# Initialize Ollama with Mistral model\n",
    "llm = OllamaLLM(\n",
    "    model=\"mistral\",\n",
    "    temperature=0.7,  # Slightly lower temperature for more focused medical responses\n",
    "    system=\"You are a medical assistant specialized in diabetes care.\"\n",
    ")\n",
    "\n",
    "@traceable()\n",
    "def rag_bot(question: str) -> dict:\n",
    "    # LangChain retriever will be automatically traced\n",
    "    docs = retriever.invoke(question)\n",
    "    docs_string = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    instructions = f\"\"\"You are DiabetIQ, an AI assistant specialized in diabetes management.\n",
    "    Use the following clinical sources to answer the patient's question accurately and safely.\n",
    "    Follow these rules strictly:\n",
    "    1. Provide only evidence-based medical information\n",
    "    2. Never give dosage advice without \"consult your doctor\" disclaimer\n",
    "    3. Flag any conflicting information in sources\n",
    "    4. Keep answers concise (2-3 sentences maximum)\n",
    "    5. If unsure, say \"I recommend consulting your healthcare provider\"\n",
    "\n",
    "    Clinical Sources:\n",
    "    {docs_string}\n",
    "\n",
    "    Patient Question: {question}\"\"\"\n",
    "\n",
    "    # Invoke Mistral via Ollama\n",
    "    response = llm.invoke(instructions)\n",
    "    \n",
    "    return {\n",
    "        \"answer\": response,\n",
    "        \"documents\": docs,\n",
    "        \"sources\": [doc.metadata.get('source', '') for doc in docs]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0d7b12a-2f94-4ab6-bf33-dbd6c4041108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['fa2e31f9-baee-4e32-a4de-dfd9747bde6c',\n",
       "  '886e39bc-50c6-4a15-afd5-bc8b70137a29',\n",
       "  '65a7e0bc-522e-426e-a3da-44023d97814c',\n",
       "  '9f1a9f38-365a-4960-967a-fa84937bed99',\n",
       "  'e32c4fae-84fe-441c-973d-a7886a9fbf50'],\n",
       " 'count': 5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What are common symptoms of low blood sugar (hypoglycemia)?\"},\n",
    "        \"outputs\": {\"answer\": \"Common symptoms of hypoglycemia include shakiness, sweating, dizziness, confusion, rapid heartbeat, hunger, and irritability. Severe cases can lead to loss of consciousness. It's important to treat it promptly.\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How can I use the DiabetIQ app to log my meals?\"},\n",
    "        \"outputs\": {\"answer\": \"In the DiabetIQ app, navigate to the 'Log' or 'Diary' section, select 'Meal', and enter details like the food items, portion sizes, estimated carbohydrates, and the time of the meal. Saving this helps track your dietary intake.\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Why is monitoring blood glucose levels important for diabetes management?\"},\n",
    "        \"outputs\": {\"answer\": \"Monitoring blood glucose helps you understand how food, activity, medication, and stress affect your levels. This information empowers you and your healthcare team to make informed decisions about your treatment plan to maintain target ranges and prevent complications.\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Can DiabetIQ help predict my risk of high blood sugar?\"},\n",
    "        \"outputs\": {\"answer\": \"DiabetIQ uses machine learning based on your logged data (like meals, activity, glucose readings) to identify patterns and potentially indicate an increased short-term risk of high blood sugar (hyperglycemia). This feature is for informational purposes to help you be proactive and should be discussed with your healthcare provider.\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What type of exercise is good for managing diabetes?\"},\n",
    "        \"outputs\": {\"answer\": \"A combination of aerobic exercise (like brisk walking, swimming, cycling) and resistance training (like lifting weights or using resistance bands) is generally recommended. Always consult your doctor before starting any new exercise program to ensure it's safe and appropriate for you.\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Create the dataset and examples in LangSmith\n",
    "dataset_name = \"DiabetIQ &&A\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "client.create_examples(\n",
    "    dataset_id=dataset.id,\n",
    "    examples=examples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd8a71-6d1a-4e14-a1d2-efe5cd9b543b",
   "metadata": {},
   "source": [
    "# Correctness: Response vs reference answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bd20540-9b4a-41e7-8d7c-1c534305112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "# output schema\n",
    "class CorrectnessGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Detailed clinical reasoning for diabetes management accuracy\"]\n",
    "    correct: Annotated[bool, ..., \"True if answer is medically accurate for diabetes care\"]\n",
    "\n",
    "\n",
    "# prompt\n",
    "correctness_instructions = \"\"\"You are an expert evaluator assessing the factual accuracy of chatbot responses in DiabetIQ: an intelligent diabetes management application with an integrated LLM-augmented chatbot.\n",
    "\n",
    "You will be given:\n",
    "- A USER QUESTION (related to diabetes care, management, medications, symptoms, etc.)\n",
    "- A GROUND TRUTH ANSWER (clinically accurate and verified information)\n",
    "- A CHATBOT RESPONSE (generated by the DiabetIQ chatbot)\n",
    "\n",
    "Your task is to evaluate the **correctness** of the chatbot’s response according to the following criteria:\n",
    "\n",
    "(1) Evaluate only the factual accuracy of the chatbot response **relative to the ground truth**. Do not grade based on style, formatting, or tone.\n",
    "(2) Ensure the chatbot response **does not include any incorrect or misleading information** compared to the ground truth.\n",
    "(3) It is acceptable if the chatbot includes **additional medically accurate** information, as long as it does not contradict or misrepresent the ground truth.\n",
    "(4) Pay special attention to **clinical accuracy**, especially for medications, insulin usage, dietary advice, blood glucose targets, or symptom interpretation.\n",
    "\n",
    "Correctness:\n",
    "- A correctness value of **True** means the chatbot's answer is entirely consistent with and accurate relative to the ground truth.\n",
    "- A correctness value of **False** means the chatbot's answer contains inaccuracies, contradictions, or medically misleading content.\n",
    "\n",
    "Provide a **step-by-step explanation** for your judgment. Do not begin by stating the correct answer. Instead, walk through your analysis comparing the chatbot response to the ground truth carefully and clearly.\n",
    "\n",
    "Be rigorous and thoughtful in your evaluation — patient safety depends on accurate information.\n",
    "\"\"\"\n",
    "\n",
    "# LLM\n",
    "grader_llm = OllamaLLM(\n",
    "    model=\"mistral\",\n",
    "    temperature=0,\n",
    "    system=diabetes_correctness_instructions\n",
    ").with_structured_output(GroundedGrade, method=\"json_schema\", strict=True)\n",
    "\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"An evaluator for RAG answer accuracy\"\"\"\n",
    "    answers = f\"\"\"\\\n",
    "QUESTION: {inputs['question']}\n",
    "GROUND TRUTH ANSWER: {reference_outputs['answer']}\n",
    "STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
    "\n",
    "    # Run evaluator\n",
    "    grade = grader_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": correctness_instructions}, \n",
    "        {\"role\": \"user\", \"content\": answers}\n",
    "    ])\n",
    "    return grade[\"correct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee695bb4-3a5f-4176-8a7f-038978127378",
   "metadata": {},
   "source": [
    "# Relevance: Response vs input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f28b178a-47d6-43d2-b596-8bca6d1dbf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output schema\n",
    "class RelevanceGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Clinical relevance analysis for diabetes context\"]\n",
    "    relevant: Annotated[bool, ..., \"Whether answer properly addresses medical question\"]\n",
    "\n",
    "# Grade prompt\n",
    "relevance_instructions=diabetiq_relevance_instructions = \"\"\"You are an expert evaluator assessing the relevance of chatbot responses in DiabetIQ: an intelligent diabetes management application with an integrated LLM-augmented chatbot.\n",
    "\n",
    "You will be given:\n",
    "- A USER QUESTION (related to diabetes care, management, medications, lifestyle, etc.)\n",
    "- A CHATBOT RESPONSE (generated by the DiabetIQ chatbot)\n",
    "\n",
    "Your task is to evaluate the **relevance** of the chatbot’s response according to the following criteria:\n",
    "\n",
    "(1) Ensure the CHATBOT RESPONSE is **concise** and **directly relevant** to the USER QUESTION. It should stay on topic and not introduce unrelated or tangential information.\n",
    "(2) Ensure the CHATBOT RESPONSE **meaningfully helps to answer the USER QUESTION**, either by providing actionable guidance, informative clarification, or medically appropriate context.\n",
    "\n",
    "Relevance:\n",
    "- A relevance value of **True** means the chatbot's response meets both criteria and contributes directly to answering the user’s question.\n",
    "- A relevance value of **False** means the chatbot's response is off-topic, overly verbose without added value, or does not clearly address the user's question.\n",
    "\n",
    "Provide a **step-by-step explanation** for your decision. Do not begin by stating the final judgment. Instead, walk through how the response aligns (or does not align) with the user's question to ensure your reasoning is sound and clear.\n",
    "\n",
    "This evaluation helps ensure the chatbot provides focused and helpful responses to people managing their diabetes.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# LLM\n",
    "relevance_llm = OllamaLLM(\n",
    "    model=\"mistral\",\n",
    "    temperature=0,\n",
    "    system=relevance_instructions\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluator\n",
    "def relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"A simple evaluator for RAG answer helpfulness.\"\"\"\n",
    "    answer = f\"QUESTION: {inputs['question']}\\n ANSWER: {outputs['answer']}\"\n",
    "    grade = relevance_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": relevance_instructions}, \n",
    "        {\"role\": \"user\", \"content\": answer}\n",
    "    ])\n",
    "    return grade[\"relevant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d261b0-cc17-4496-bbcc-014c4064b1c8",
   "metadata": {},
   "source": [
    "# Groundedness: Response vs retrieved docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6425e852-03c1-4afa-a4ea-315e50551260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade output schema\n",
    "class GroundedGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    grounded: Annotated[bool, ..., \"Provide the score on if the answer hallucinates from the documents\"]\n",
    "\n",
    "# Grade prompt\n",
    "grounded_instructions = diabetiq_groundedness_instructions = \"\"\"You are an expert evaluator assessing the groundedness of chatbot responses in DiabetIQ: an intelligent diabetes management application with an integrated LLM-augmented chatbot.\n",
    "\n",
    "You will be given:\n",
    "- A set of FACTS (retrieved evidence, clinical references, or authoritative information)\n",
    "- A CHATBOT RESPONSE (generated by the DiabetIQ chatbot)\n",
    "\n",
    "Your task is to evaluate the **groundedness** of the chatbot’s response according to the following criteria:\n",
    "\n",
    "(1) Ensure the CHATBOT RESPONSE is **based on and supported by the provided FACTS**.\n",
    "(2) Ensure the CHATBOT RESPONSE does **not include hallucinated or fabricated information** that is not supported by or inferable from the FACTS.\n",
    "\n",
    "Grounded:\n",
    "- A grounded value of **True** means the chatbot's response remains strictly within the bounds of the provided FACTS and is properly supported by them.\n",
    "- A grounded value of **False** means the chatbot's response includes unsupported claims, makes assumptions not backed by the facts, or introduces external/hallucinated information.\n",
    "\n",
    "Explain your reasoning in a **step-by-step** manner to ensure your conclusion is accurate and well-justified. Focus on whether each part of the chatbot response can be traced back to the given facts.\n",
    "\n",
    "This evaluation is crucial to ensure the chatbot provides users with reliable, evidence-based diabetes management information.\n",
    "\"\"\"\n",
    "\n",
    "# LLM \n",
    "grounded_llm =  OllamaLLM(\n",
    "    model=\"mistral\",\n",
    "    temperature=0,\n",
    "    system=relevance_instructions\n",
    ")\n",
    "\n",
    "# Evaluator\n",
    "def groundedness(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"A simple evaluator for RAG answer groundedness.\"\"\"\n",
    "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
    "    answer = f\"FACTS: {doc_string}\\nANSWER: {outputs['answer']}\"\n",
    "    grade = grounded_llm.invoke([{\"role\": \"system\", \"content\": grounded_instructions}, {\"role\": \"user\", \"content\": answer}])\n",
    "    return grade[\"grounded\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba331b1-4449-459a-8811-bbcf844ac6de",
   "metadata": {},
   "source": [
    "# Retrieval relevance: Retrieved docs vs input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58180527-0758-445f-953d-f82aa922a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade output schema\n",
    "class RetrievalRelevanceGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    relevant: Annotated[bool, ..., \"True if the retrieved documents are relevant to the question, False otherwise\"]\n",
    "\n",
    "# Grade prompt\n",
    "retrieval_relevance_instructions = \"\"\"You are an expert evaluator assessing the relevance of retrieved information used by DiabetIQ: an intelligent diabetes management application with an integrated LLM-augmented chatbot.\n",
    "\n",
    "You will be given:\n",
    "- A USER QUESTION (related to diabetes care, treatment, lifestyle, or symptoms)\n",
    "- A set of FACTS (documents or text segments retrieved by the system to support answering the question)\n",
    "\n",
    "Your task is to evaluate the **retrieval relevance** of the FACTS using the following criteria:\n",
    "\n",
    "(1) Your goal is to identify FACTS that are **completely unrelated** to the USER QUESTION.\n",
    "(2) If the FACTS contain **any keywords or semantic meaning related to the QUESTION**, consider them relevant.\n",
    "(3) It is acceptable if the FACTS include some unrelated information, as long as the overall content is topically or semantically related to the question.\n",
    "\n",
    "Relevance:\n",
    "- A relevance value of **True** means the FACTS contain **any** keywords, concepts, or medically relevant semantic meaning tied to the USER QUESTION and are therefore relevant.\n",
    "- A relevance value of **False** means the FACTS are **entirely unrelated** to the USER QUESTION and offer no value in helping answer it.\n",
    "\n",
    "Provide a **step-by-step explanation** of your reasoning. Begin by analyzing the QUESTION and the FACTS, then determine if any part of the retrieved content helps address the question, even partially or indirectly.\n",
    "\n",
    "This evaluation ensures that DiabetIQ retrieves clinically meaningful information that aligns with user needs.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Grader LLM\n",
    "retrieval_relevance_llm = OllamaLLM(\n",
    "    model=\"mistral\",\n",
    "    temperature=0,\n",
    "    system=relevance_instructions\n",
    ")\n",
    "\n",
    "def retrieval_relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"An evaluator for document relevance\"\"\"\n",
    "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
    "    answer = f\"FACTS: {doc_string}\\nQUESTION: {inputs['question']}\"\n",
    "\n",
    "    # Run evaluator\n",
    "    grade = retrieval_relevance_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": retrieval_relevance_instructions}, \n",
    "        {\"role\": \"user\", \"content\": answer}\n",
    "    ])\n",
    "    return grade[\"relevant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b44a6-a31f-40be-b259-b98159179ad9",
   "metadata": {},
   "source": [
    "# Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c1e93a4-0f1c-463f-9f2a-d0593709d4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-doc-relevance-d5201a91' at:\n",
      "https://smith.langchain.com/o/ff343bf1-33c1-5f3a-8dc5-4f923eff2848/datasets/1315b3c3-c3dd-46ca-bfc4-5044736dac09/compare?selectedSessions=3ec7289a-ef60-4185-8b2e-adc2abfab292\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6431487a0f9b4758bbff4bcbea2f3e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error getting collection: Database error: error returned from database: (code: 1) no such table: collections\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1915, in _forward\n",
      "    fn(\n",
      "    ~~^\n",
      "        *args,\n",
      "        ^^^^^^\n",
      "        langsmith_extra=langsmith_extra,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3127263961.py\", line 2, in target\n",
      "    return rag_bot(inputs[\"question\"])\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3902628271.py\", line 13, in rag_bot\n",
      "    docs = retriever.invoke(question)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_core\\retrievers.py\", line 259, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "        input, run_manager=run_manager, **_kwargs\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py\", line 1077, in _get_relevant_documents\n",
      "    docs = self.vectorstore.similarity_search(query, **_kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 350, in similarity_search\n",
      "    docs_and_scores = self.similarity_search_with_score(\n",
      "        query, k, filter=filter, **kwargs\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 440, in similarity_search_with_score\n",
      "    results = self.__query_collection(\n",
      "        query_embeddings=[query_embedding],\n",
      "    ...<3 lines>...\n",
      "        **kwargs,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_core\\utils\\utils.py\", line 53, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 157, in __query_collection\n",
      "    return self._collection.query(  # type: ignore[return-value]\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        query_texts=query_texts,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py\", line 219, in query\n",
      "    query_results = self._client._query(\n",
      "        collection_id=self.id,\n",
      "    ...<6 lines>...\n",
      "        database=self.database,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\chromadb\\api\\rust.py\", line 493, in _query\n",
      "    rust_response = self.bindings.query(\n",
      "        str(collection_id),\n",
      "    ...<6 lines>...\n",
      "        database,\n",
      "    )\n",
      "chromadb.errors.InternalError: Error getting collection: Database error: error returned from database: (code: 1) no such table: collections\n",
      "Error running evaluator <DynamicRunEvaluator correctness> on run 02e2ac90-f6b9-4a02-9e75-ec2b2d221c95: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3530014555.py\", line 45, in correctness\n",
      "    STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
      "                     ~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator groundedness> on run 02e2ac90-f6b9-4a02-9e75-ec2b2d221c95: KeyError('documents')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3497266911.py\", line 37, in groundedness\n",
      "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
      "                                                         ~~~~~~~^^^^^^^^^^^^^\n",
      "KeyError: 'documents'\n",
      "Error running evaluator <DynamicRunEvaluator relevance> on run 02e2ac90-f6b9-4a02-9e75-ec2b2d221c95: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\4193301577.py\", line 38, in relevance\n",
      "    answer = f\"QUESTION: {inputs['question']}\\n ANSWER: {outputs['answer']}\"\n",
      "                                                         ~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator retrieval_relevance> on run 02e2ac90-f6b9-4a02-9e75-ec2b2d221c95: KeyError('documents')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\2980715023.py\", line 38, in retrieval_relevance\n",
      "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
      "                                                         ~~~~~~~^^^^^^^^^^^^^\n",
      "KeyError: 'documents'\n",
      "Error running target function: Error getting collection: Database error: error returned from database: (code: 1) no such table: collections\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1915, in _forward\n",
      "    fn(\n",
      "    ~~^\n",
      "        *args,\n",
      "        ^^^^^^\n",
      "        langsmith_extra=langsmith_extra,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3127263961.py\", line 2, in target\n",
      "    return rag_bot(inputs[\"question\"])\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3902628271.py\", line 13, in rag_bot\n",
      "    docs = retriever.invoke(question)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_core\\retrievers.py\", line 259, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "        input, run_manager=run_manager, **_kwargs\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py\", line 1077, in _get_relevant_documents\n",
      "    docs = self.vectorstore.similarity_search(query, **_kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 350, in similarity_search\n",
      "    docs_and_scores = self.similarity_search_with_score(\n",
      "        query, k, filter=filter, **kwargs\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 440, in similarity_search_with_score\n",
      "    results = self.__query_collection(\n",
      "        query_embeddings=[query_embedding],\n",
      "    ...<3 lines>...\n",
      "        **kwargs,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_core\\utils\\utils.py\", line 53, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 157, in __query_collection\n",
      "    return self._collection.query(  # type: ignore[return-value]\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        query_texts=query_texts,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py\", line 219, in query\n",
      "    query_results = self._client._query(\n",
      "        collection_id=self.id,\n",
      "    ...<6 lines>...\n",
      "        database=self.database,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\chromadb\\api\\rust.py\", line 493, in _query\n",
      "    rust_response = self.bindings.query(\n",
      "        str(collection_id),\n",
      "    ...<6 lines>...\n",
      "        database,\n",
      "    )\n",
      "chromadb.errors.InternalError: Error getting collection: Database error: error returned from database: (code: 1) no such table: collections\n",
      "Error running evaluator <DynamicRunEvaluator correctness> on run da30891b-4a57-4f63-8cb8-09b5831727e5: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3530014555.py\", line 45, in correctness\n",
      "    STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
      "                     ~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator groundedness> on run da30891b-4a57-4f63-8cb8-09b5831727e5: KeyError('documents')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3497266911.py\", line 37, in groundedness\n",
      "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
      "                                                         ~~~~~~~^^^^^^^^^^^^^\n",
      "KeyError: 'documents'\n",
      "Error running evaluator <DynamicRunEvaluator relevance> on run da30891b-4a57-4f63-8cb8-09b5831727e5: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\4193301577.py\", line 38, in relevance\n",
      "    answer = f\"QUESTION: {inputs['question']}\\n ANSWER: {outputs['answer']}\"\n",
      "                                                         ~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator retrieval_relevance> on run da30891b-4a57-4f63-8cb8-09b5831727e5: KeyError('documents')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\2980715023.py\", line 38, in retrieval_relevance\n",
      "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
      "                                                         ~~~~~~~^^^^^^^^^^^^^\n",
      "KeyError: 'documents'\n",
      "Error running target function: Error getting collection: Database error: error returned from database: (code: 1) no such table: collections\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1915, in _forward\n",
      "    fn(\n",
      "    ~~^\n",
      "        *args,\n",
      "        ^^^^^^\n",
      "        langsmith_extra=langsmith_extra,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3127263961.py\", line 2, in target\n",
      "    return rag_bot(inputs[\"question\"])\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3902628271.py\", line 13, in rag_bot\n",
      "    docs = retriever.invoke(question)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_core\\retrievers.py\", line 259, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "        input, run_manager=run_manager, **_kwargs\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py\", line 1077, in _get_relevant_documents\n",
      "    docs = self.vectorstore.similarity_search(query, **_kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 350, in similarity_search\n",
      "    docs_and_scores = self.similarity_search_with_score(\n",
      "        query, k, filter=filter, **kwargs\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 440, in similarity_search_with_score\n",
      "    results = self.__query_collection(\n",
      "        query_embeddings=[query_embedding],\n",
      "    ...<3 lines>...\n",
      "        **kwargs,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_core\\utils\\utils.py\", line 53, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 157, in __query_collection\n",
      "    return self._collection.query(  # type: ignore[return-value]\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        query_texts=query_texts,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py\", line 219, in query\n",
      "    query_results = self._client._query(\n",
      "        collection_id=self.id,\n",
      "    ...<6 lines>...\n",
      "        database=self.database,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\chromadb\\api\\rust.py\", line 493, in _query\n",
      "    rust_response = self.bindings.query(\n",
      "        str(collection_id),\n",
      "    ...<6 lines>...\n",
      "        database,\n",
      "    )\n",
      "chromadb.errors.InternalError: Error getting collection: Database error: error returned from database: (code: 1) no such table: collections\n",
      "Error running evaluator <DynamicRunEvaluator correctness> on run 3fda58a5-55a1-4f5f-ae51-bc8624e7ef30: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3530014555.py\", line 45, in correctness\n",
      "    STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
      "                     ~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator groundedness> on run 3fda58a5-55a1-4f5f-ae51-bc8624e7ef30: KeyError('documents')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3497266911.py\", line 37, in groundedness\n",
      "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
      "                                                         ~~~~~~~^^^^^^^^^^^^^\n",
      "KeyError: 'documents'\n",
      "Error running evaluator <DynamicRunEvaluator relevance> on run 3fda58a5-55a1-4f5f-ae51-bc8624e7ef30: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\4193301577.py\", line 38, in relevance\n",
      "    answer = f\"QUESTION: {inputs['question']}\\n ANSWER: {outputs['answer']}\"\n",
      "                                                         ~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator retrieval_relevance> on run 3fda58a5-55a1-4f5f-ae51-bc8624e7ef30: KeyError('documents')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\2980715023.py\", line 38, in retrieval_relevance\n",
      "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
      "                                                         ~~~~~~~^^^^^^^^^^^^^\n",
      "KeyError: 'documents'\n",
      "Error running target function: Error getting collection: Database error: error returned from database: (code: 1) no such table: collections\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1915, in _forward\n",
      "    fn(\n",
      "    ~~^\n",
      "        *args,\n",
      "        ^^^^^^\n",
      "        langsmith_extra=langsmith_extra,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3127263961.py\", line 2, in target\n",
      "    return rag_bot(inputs[\"question\"])\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3902628271.py\", line 13, in rag_bot\n",
      "    docs = retriever.invoke(question)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_core\\retrievers.py\", line 259, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "        input, run_manager=run_manager, **_kwargs\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py\", line 1077, in _get_relevant_documents\n",
      "    docs = self.vectorstore.similarity_search(query, **_kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 350, in similarity_search\n",
      "    docs_and_scores = self.similarity_search_with_score(\n",
      "        query, k, filter=filter, **kwargs\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 440, in similarity_search_with_score\n",
      "    results = self.__query_collection(\n",
      "        query_embeddings=[query_embedding],\n",
      "    ...<3 lines>...\n",
      "        **kwargs,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_core\\utils\\utils.py\", line 53, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 157, in __query_collection\n",
      "    return self._collection.query(  # type: ignore[return-value]\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        query_texts=query_texts,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py\", line 219, in query\n",
      "    query_results = self._client._query(\n",
      "        collection_id=self.id,\n",
      "    ...<6 lines>...\n",
      "        database=self.database,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\chromadb\\api\\rust.py\", line 493, in _query\n",
      "    rust_response = self.bindings.query(\n",
      "        str(collection_id),\n",
      "    ...<6 lines>...\n",
      "        database,\n",
      "    )\n",
      "chromadb.errors.InternalError: Error getting collection: Database error: error returned from database: (code: 1) no such table: collections\n",
      "Error running evaluator <DynamicRunEvaluator correctness> on run 52534fc6-a3cd-465e-b9c9-919db2788778: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3530014555.py\", line 45, in correctness\n",
      "    STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
      "                     ~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator groundedness> on run 52534fc6-a3cd-465e-b9c9-919db2788778: KeyError('documents')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3497266911.py\", line 37, in groundedness\n",
      "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
      "                                                         ~~~~~~~^^^^^^^^^^^^^\n",
      "KeyError: 'documents'\n",
      "Error running evaluator <DynamicRunEvaluator relevance> on run 52534fc6-a3cd-465e-b9c9-919db2788778: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\4193301577.py\", line 38, in relevance\n",
      "    answer = f\"QUESTION: {inputs['question']}\\n ANSWER: {outputs['answer']}\"\n",
      "                                                         ~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator retrieval_relevance> on run 52534fc6-a3cd-465e-b9c9-919db2788778: KeyError('documents')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\2980715023.py\", line 38, in retrieval_relevance\n",
      "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
      "                                                         ~~~~~~~^^^^^^^^^^^^^\n",
      "KeyError: 'documents'\n",
      "Error running target function: Error getting collection: Database error: error returned from database: (code: 1) no such table: collections\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1915, in _forward\n",
      "    fn(\n",
      "    ~~^\n",
      "        *args,\n",
      "        ^^^^^^\n",
      "        langsmith_extra=langsmith_extra,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3127263961.py\", line 2, in target\n",
      "    return rag_bot(inputs[\"question\"])\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3902628271.py\", line 13, in rag_bot\n",
      "    docs = retriever.invoke(question)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_core\\retrievers.py\", line 259, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "        input, run_manager=run_manager, **_kwargs\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py\", line 1077, in _get_relevant_documents\n",
      "    docs = self.vectorstore.similarity_search(query, **_kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 350, in similarity_search\n",
      "    docs_and_scores = self.similarity_search_with_score(\n",
      "        query, k, filter=filter, **kwargs\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 440, in similarity_search_with_score\n",
      "    results = self.__query_collection(\n",
      "        query_embeddings=[query_embedding],\n",
      "    ...<3 lines>...\n",
      "        **kwargs,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_core\\utils\\utils.py\", line 53, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 157, in __query_collection\n",
      "    return self._collection.query(  # type: ignore[return-value]\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        query_texts=query_texts,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py\", line 219, in query\n",
      "    query_results = self._client._query(\n",
      "        collection_id=self.id,\n",
      "    ...<6 lines>...\n",
      "        database=self.database,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\chromadb\\api\\rust.py\", line 493, in _query\n",
      "    rust_response = self.bindings.query(\n",
      "        str(collection_id),\n",
      "    ...<6 lines>...\n",
      "        database,\n",
      "    )\n",
      "chromadb.errors.InternalError: Error getting collection: Database error: error returned from database: (code: 1) no such table: collections\n",
      "Error running evaluator <DynamicRunEvaluator correctness> on run 81c01d25-6dec-4978-a82d-d948ff8f63d1: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3530014555.py\", line 45, in correctness\n",
      "    STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
      "                     ~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator groundedness> on run 81c01d25-6dec-4978-a82d-d948ff8f63d1: KeyError('documents')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\3497266911.py\", line 37, in groundedness\n",
      "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
      "                                                         ~~~~~~~^^^^^^^^^^^^^\n",
      "KeyError: 'documents'\n",
      "Error running evaluator <DynamicRunEvaluator relevance> on run 81c01d25-6dec-4978-a82d-d948ff8f63d1: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\4193301577.py\", line 38, in relevance\n",
      "    answer = f\"QUESTION: {inputs['question']}\\n ANSWER: {outputs['answer']}\"\n",
      "                                                         ~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator retrieval_relevance> on run 81c01d25-6dec-4978-a82d-d948ff8f63d1: KeyError('documents')\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 635, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"E:\\DiabetIQ\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 744, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14536\\2980715023.py\", line 38, in retrieval_relevance\n",
      "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
      "                                                         ~~~~~~~^^^^^^^^^^^^^\n",
      "KeyError: 'documents'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>feedback.wrapper</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why is monitoring blood glucose levels importa...</td>\n",
       "      <td>None</td>\n",
       "      <td>InternalError('Error getting collection: Datab...</td>\n",
       "      <td>Monitoring blood glucose helps you understand ...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.016651</td>\n",
       "      <td>65a7e0bc-522e-426e-a3da-44023d97814c</td>\n",
       "      <td>02e2ac90-f6b9-4a02-9e75-ec2b2d221c95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I use the DiabetIQ app to log my meals?</td>\n",
       "      <td>None</td>\n",
       "      <td>InternalError('Error getting collection: Datab...</td>\n",
       "      <td>In the DiabetIQ app, navigate to the 'Log' or ...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.015276</td>\n",
       "      <td>886e39bc-50c6-4a15-afd5-bc8b70137a29</td>\n",
       "      <td>da30891b-4a57-4f63-8cb8-09b5831727e5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can DiabetIQ help predict my risk of high bloo...</td>\n",
       "      <td>None</td>\n",
       "      <td>InternalError('Error getting collection: Datab...</td>\n",
       "      <td>DiabetIQ uses machine learning based on your l...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.014721</td>\n",
       "      <td>9f1a9f38-365a-4960-967a-fa84937bed99</td>\n",
       "      <td>3fda58a5-55a1-4f5f-ae51-bc8624e7ef30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What type of exercise is good for managing dia...</td>\n",
       "      <td>None</td>\n",
       "      <td>InternalError('Error getting collection: Datab...</td>\n",
       "      <td>A combination of aerobic exercise (like brisk ...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.014764</td>\n",
       "      <td>e32c4fae-84fe-441c-973d-a7886a9fbf50</td>\n",
       "      <td>52534fc6-a3cd-465e-b9c9-919db2788778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are common symptoms of low blood sugar (h...</td>\n",
       "      <td>None</td>\n",
       "      <td>InternalError('Error getting collection: Datab...</td>\n",
       "      <td>Common symptoms of hypoglycemia include shakin...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.014338</td>\n",
       "      <td>fa2e31f9-baee-4e32-a4de-dfd9747bde6c</td>\n",
       "      <td>81c01d25-6dec-4978-a82d-d948ff8f63d1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     inputs.question outputs.output  \\\n",
       "0  Why is monitoring blood glucose levels importa...           None   \n",
       "1    How can I use the DiabetIQ app to log my meals?           None   \n",
       "2  Can DiabetIQ help predict my risk of high bloo...           None   \n",
       "3  What type of exercise is good for managing dia...           None   \n",
       "4  What are common symptoms of low blood sugar (h...           None   \n",
       "\n",
       "                                               error  \\\n",
       "0  InternalError('Error getting collection: Datab...   \n",
       "1  InternalError('Error getting collection: Datab...   \n",
       "2  InternalError('Error getting collection: Datab...   \n",
       "3  InternalError('Error getting collection: Datab...   \n",
       "4  InternalError('Error getting collection: Datab...   \n",
       "\n",
       "                                    reference.answer feedback.wrapper  \\\n",
       "0  Monitoring blood glucose helps you understand ...             None   \n",
       "1  In the DiabetIQ app, navigate to the 'Log' or ...             None   \n",
       "2  DiabetIQ uses machine learning based on your l...             None   \n",
       "3  A combination of aerobic exercise (like brisk ...             None   \n",
       "4  Common symptoms of hypoglycemia include shakin...             None   \n",
       "\n",
       "   execution_time                            example_id  \\\n",
       "0        0.016651  65a7e0bc-522e-426e-a3da-44023d97814c   \n",
       "1        0.015276  886e39bc-50c6-4a15-afd5-bc8b70137a29   \n",
       "2        0.014721  9f1a9f38-365a-4960-967a-fa84937bed99   \n",
       "3        0.014764  e32c4fae-84fe-441c-973d-a7886a9fbf50   \n",
       "4        0.014338  fa2e31f9-baee-4e32-a4de-dfd9747bde6c   \n",
       "\n",
       "                                     id  \n",
       "0  02e2ac90-f6b9-4a02-9e75-ec2b2d221c95  \n",
       "1  da30891b-4a57-4f63-8cb8-09b5831727e5  \n",
       "2  3fda58a5-55a1-4f5f-ae51-bc8624e7ef30  \n",
       "3  52534fc6-a3cd-465e-b9c9-919db2788778  \n",
       "4  81c01d25-6dec-4978-a82d-d948ff8f63d1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def target(inputs: dict) -> dict:\n",
    "    return rag_bot(inputs[\"question\"])\n",
    "\n",
    "experiment_results = client.evaluate(\n",
    "    target,\n",
    "    data=dataset_name,\n",
    "    evaluators=[correctness, groundedness, relevance, retrieval_relevance],\n",
    "    experiment_prefix=\"rag-doc-relevance\",\n",
    "    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},\n",
    ")\n",
    "\n",
    "experiment_results.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
