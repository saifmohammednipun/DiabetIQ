{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f7d318-5ba0-482b-b620-2314488db839",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a59080d6-a0da-4d29-9e94-64370bc648c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Ensure the environment variables are set\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "huggingface_api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "if not langchain_api_key:\n",
    "    raise ValueError(\"LANGCHAIN_API_KEY is not set in the environment variables.\")\n",
    "if not huggingface_api_key:\n",
    "    raise ValueError(\"HUGGINGFACE_API_KEY is not set in the environment variables.\")\n",
    "\n",
    "# Set environment variables for the application\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key\n",
    "os.environ['HUGGINGFACE_API_KEY'] = huggingface_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9dc62f85-f49f-4ef8-8607-5535ce45105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0210a5-6751-466d-aa94-d99d35fc8cd1",
   "metadata": {},
   "source": [
    "# Load Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d7ef2ee-70b3-43de-ae31-f6cb96291285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DIABETES CARE \\nBADAS Guideline 2019 \\n          \\n  \\n   \\n  \\n   P|) \\nDAS GUELINE ON Man \\nDELIT IGEMEN \\n  \\nA Joint Initiative of \\nDiabetic Association of Bangladesh \\nNCDC Program, Directorate General of Health Services'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Documents (use PyPDFLoader for PDF)\n",
    "file_path = r\"C:\\Users\\User\\Desktop\\NSU\\CSE299 Materials\\LLM\\Dataset\\Diabetes_Care_BADAS_guideline2019-3.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "docs[0].page_content[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2fd034-6943-44ab-b80a-0a560ec877ac",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df597a38-fb38-4885-babf-50ff64cf26e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "DIABETES CARE \n",
      "BADAS Guideline 2019 \n",
      "          \n",
      "  \n",
      "   \n",
      "  \n",
      "   P|) \n",
      "DAS GUELINE ON Man \n",
      "DELIT IGEMEN \n",
      "  \n",
      "A Joint Initiative of \n",
      "Diabetic Association of Bangladesh \n",
      "NCDC Program, Directorate General of Health Services\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--- Chunk 2 ---\n",
      "DIABETES CARE \n",
      "BADAS Guideline 2019 \n",
      "  \n",
      "A Joint Initiative of \n",
      "Diabetic Association of Bangladesh \n",
      "NCDC Program, Directorate General of Health Services \n",
      "  \n",
      "Diabetes Care: BADAS Guideline 2019 HEI! 1\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--- Chunk 3 ---\n",
      "DIABETES CARE: BADAS GUIDELINE 2019 \n",
      "Convener: Prof A K Azad Khan \n",
      "Chairman: Prof Hajera Mahtab \n",
      "Members of the steering committee \n",
      "Prof Dr AHM Enayet Hossain \n",
      "Prof Akhtar Hussain \n",
      "Prof Zafar Anmed Latif \n",
      "Prof Tofail Ahmed \n",
      "Prof Laique Ahmed Khan \n",
      "Prof Nazrul Islam Siddiqui \n",
      "Prof Md Hafizur Rahman \n",
      "Prof Abdus Saleque Mollah \n",
      "Prof Md Farid Uddin \n",
      "Prof M A Jalil Ansary \n",
      "Prof Dr MA Samad \n",
      "Prof SM Ashrafuzzaman \n",
      "Prof MA Hasnat \n",
      "Dr Kazi Ali Hassan \n",
      "Dr Abdul Mannan Sarker \n",
      "Members of the Task Force \n",
      "Prof Md Faruque Pathan \n",
      "Dr Tareen Ahmed \n",
      "Dr Md Feroz Amin \n",
      "Dr Faria Afsana \n",
      "Dr Bishwajit Bhowmik \n",
      "Dr Nazmul Kabir Qureshi \n",
      "Dr Indrajit Prasad \n",
      "Dr Tanjina Hossain \n",
      "Dr Shahjada Selim \n",
      "Dr M Saifuddin \n",
      "Dr Tasnima Siddiquee \n",
      "Dr Abdul Alim \n",
      "Published \n",
      "November 2019 \n",
      "Correspondence \n",
      "Diabetic Association of Bangladesh \n",
      "122 Kazi Nazrul Islam Avenue, Dhaka 1000, Bangladesh \n",
      "Design and layout \n",
      "Liason Office,122 Kazi Nazrul Islam avenue, Shabag, Dhaka \n",
      "Printing \n",
      "BADAS-RVTC Printing Press\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "for i, chunk in enumerate(splits[:3]):  # Show the first 3 chunks\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(chunk.page_content[:1000])  # Print the first 1000 characters of the chunk\n",
    "    print(\"\\n\" + \"-\"*70 + \"\\n\")  # Separator between chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bf2a0a-f4c7-420a-8867-5762783b8946",
   "metadata": {},
   "source": [
    "# Count token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c800bdef-5aa4-45b1-873b-8e71148dc3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "Text snippet (first 1000 characters):\n",
      "DIABETES CARE \n",
      "BADAS Guideline 2019 \n",
      "          \n",
      "  \n",
      "   \n",
      "  \n",
      "   P|) \n",
      "DAS GUELINE ON Man \n",
      "DELIT IGEMEN \n",
      "  \n",
      "A Joint Initiative of \n",
      "Diabetic Association of Bangladesh \n",
      "NCDC Program, Directorate General of Health Services\n",
      "Number of tokens in this chunk: 57\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Text snippet (first 1000 characters):\n",
      "DIABETES CARE \n",
      "BADAS Guideline 2019 \n",
      "  \n",
      "A Joint Initiative of \n",
      "Diabetic Association of Bangladesh \n",
      "NCDC Program, Directorate General of Health Services \n",
      "  \n",
      "Diabetes Care: BADAS Guideline 2019 HEI! 1\n",
      "Number of tokens in this chunk: 51\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Text snippet (first 1000 characters):\n",
      "DIABETES CARE: BADAS GUIDELINE 2019 \n",
      "Convener: Prof A K Azad Khan \n",
      "Chairman: Prof Hajera Mahtab \n",
      "Members of the steering committee \n",
      "Prof Dr AHM Enayet Hossain \n",
      "Prof Akhtar Hussain \n",
      "Prof Zafar Anmed Latif \n",
      "Prof Tofail Ahmed \n",
      "Prof Laique Ahmed Khan \n",
      "Prof Nazrul Islam Siddiqui \n",
      "Prof Md Hafizur Rahman \n",
      "Prof Abdus Saleque Mollah \n",
      "Prof Md Farid Uddin \n",
      "Prof M A Jalil Ansary \n",
      "Prof Dr MA Samad \n",
      "Prof SM Ashrafuzzaman \n",
      "Prof MA Hasnat \n",
      "Dr Kazi Ali Hassan \n",
      "Dr Abdul Mannan Sarker \n",
      "Members of the Task Force \n",
      "Prof Md Faruque Pathan \n",
      "Dr Tareen Ahmed \n",
      "Dr Md Feroz Amin \n",
      "Dr Faria Afsana \n",
      "Dr Bishwajit Bhowmik \n",
      "Dr Nazmul Kabir Qureshi \n",
      "Dr Indrajit Prasad \n",
      "Dr Tanjina Hossain \n",
      "Dr Shahjada Selim \n",
      "Dr M Saifuddin \n",
      "Dr Tasnima Siddiquee \n",
      "Dr Abdul Alim \n",
      "Published \n",
      "November 2019 \n",
      "Correspondence \n",
      "Diabetic Association of Bangladesh \n",
      "122 Kazi Nazrul Islam Avenue, Dhaka 1000, Bangladesh \n",
      "Design and layout \n",
      "Liason Office,122 Kazi Nazrul Islam avenue, Shabag, Dhaka \n",
      "Printing \n",
      "BADAS-RVTC Printing Press\n",
      "Number of tokens in this chunk: 316\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Function to count tokens in a string using a specified encoding\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "# Use the same text_splitter from before (already done)\n",
    "# Now apply tokenization to each chunk\n",
    "\n",
    "encoding_name = \"cl100k_base\"  # The encoding used by OpenAI models like GPT-4, GPT-3.5\n",
    "\n",
    "# Tokenize each chunk and print the number of tokens\n",
    "for i, chunk in enumerate(splits[:3]):  # Show the first 3 chunks as an example\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    chunk_text = chunk.page_content  # Get the text content of the chunk\n",
    "    num_tokens = num_tokens_from_string(chunk_text, encoding_name)  # Tokenize and count tokens\n",
    "    print(f\"Text snippet (first 1000 characters):\\n{chunk_text[:1000]}\")  # Show part of the chunk\n",
    "    print(f\"Number of tokens in this chunk: {num_tokens}\")\n",
    "    print(\"\\n\" + \"-\"*70 + \"\\n\")  # Separator between chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66bf67a-3760-47cb-b4ed-fec285db1990",
   "metadata": {},
   "source": [
    "# Sentence-Transformers Models (Hugging Face)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a054486b-064f-4078-8327-875553334e83",
   "metadata": {},
   "source": [
    "## all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9be2a93-51d3-44c4-9b4f-331daeda72c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import tiktoken\n",
    "\n",
    "question = \"What is Pathophysiology?\"\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# Embed a single query\n",
    "query_result = embedding_model.embed_query(question)  \n",
    "\n",
    "# Embed multiple documents (convert to text first)\n",
    "document_result = embedding_model.embed_documents([doc.page_content for doc in docs])\n",
    "\n",
    "# Check the length of query embedding\n",
    "print(len(query_result))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4f142-618e-4296-8508-8d7986a7c538",
   "metadata": {},
   "source": [
    "# Cosine Similarity for all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9e0fa33-78bc-4ffe-b997-75ba2b2b88cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Similar Documents:\n",
      "1. Document 15 - Similarity: 0.4113\n",
      "2. Document 11 - Similarity: 0.2673\n",
      "3. Document 34 - Similarity: 0.2346\n",
      "4. Document 13 - Similarity: 0.2219\n",
      "5. Document 8 - Similarity: 0.2194\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec2, vec1)  # (79, 384) @ (384,) → (79,)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2, axis=1)  # Compute norms for each document\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Convert document embeddings to NumPy array\n",
    "document_embeddings = np.array(document_result)  # Shape: (79, 384)\n",
    "\n",
    "# Compute cosine similarity for each document\n",
    "similarities = cosine_similarity(query_result, document_embeddings)\n",
    "\n",
    "# Get the top 5 most similar documents\n",
    "top_indices = np.argsort(similarities)[::-1][:5]  # Sort in descending order and take top 5\n",
    "\n",
    "# Print results\n",
    "print(\"Top 5 Similar Documents:\")\n",
    "for i, idx in enumerate(top_indices):\n",
    "    print(f\"{i+1}. Document {idx} - Similarity: {similarities[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc90a8-2ef8-43c2-b24c-bfa6664dabb9",
   "metadata": {},
   "source": [
    "## paraphrase-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ebed9df-94e4-4dbc-9d3d-da79dc31bab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import tiktoken\n",
    "\n",
    "question = \"What is Pathophysiology?\"\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# Embed a single query\n",
    "query_result = embedding_model.embed_query(question)  \n",
    "\n",
    "# Embed multiple documents (convert to text first)\n",
    "document_result = embedding_model.embed_documents([doc.page_content for doc in docs])\n",
    "\n",
    "# Check the length of query embedding\n",
    "print(len(query_result))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a4d20f-8c61-48d2-bfca-99be6ea16009",
   "metadata": {},
   "source": [
    "# Cosine Similarity for paraphrase-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4f936fe-f217-4550-a0c5-27aa146b4663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Similar Documents:\n",
      "1. Document 11 - Similarity: 0.3470\n",
      "2. Document 63 - Similarity: 0.3361\n",
      "3. Document 15 - Similarity: 0.3287\n",
      "4. Document 32 - Similarity: 0.2957\n",
      "5. Document 35 - Similarity: 0.2831\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec2, vec1)  # (79, 384) @ (384,) → (79,)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2, axis=1)  # Compute norms for each document\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Convert document embeddings to NumPy array\n",
    "document_embeddings = np.array(document_result)  # Shape: (79, 384)\n",
    "\n",
    "# Compute cosine similarity for each document\n",
    "similarities = cosine_similarity(query_result, document_embeddings)\n",
    "\n",
    "# Get the top 5 most similar documents\n",
    "top_indices = np.argsort(similarities)[::-1][:5]  # Sort in descending order and take top 5\n",
    "\n",
    "# Print results\n",
    "print(\"Top 5 Similar Documents:\")\n",
    "for i, idx in enumerate(top_indices):\n",
    "    print(f\"{i+1}. Document {idx} - Similarity: {similarities[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf3b1ed-00e3-450b-b446-cbdefaafe569",
   "metadata": {},
   "source": [
    "# OpenAI-Compatible Open-Source Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b6538-3e3e-4ce6-b4a5-27d75e175ae9",
   "metadata": {},
   "source": [
    "## BAAI/bge-small-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "947d6899-fb6e-4770-ac4d-0166883b6f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import tiktoken\n",
    "\n",
    "question = \"What is Pathophysiology?\"\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "\n",
    "# Embed a single query\n",
    "query_result = embedding_model.embed_query(question)  \n",
    "\n",
    "# Embed multiple documents (convert to text first)\n",
    "document_result = embedding_model.embed_documents([doc.page_content for doc in docs])\n",
    "\n",
    "# Check the length of query embedding\n",
    "print(len(query_result))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dd728d-f193-4e4c-a11d-18dcf5f0e741",
   "metadata": {},
   "source": [
    "# Cosine Similarity for BAAI/bge-small-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ced5eac-7558-4f81-9e6c-c99265e0c8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Similar Documents:\n",
      "1. Document 9 - Similarity: 0.8366\n",
      "2. Document 15 - Similarity: 0.8105\n",
      "3. Document 10 - Similarity: 0.8023\n",
      "4. Document 11 - Similarity: 0.8023\n",
      "5. Document 46 - Similarity: 0.7908\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec2, vec1)  # (79, 384) @ (384,) → (79,)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2, axis=1)  # Compute norms for each document\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Convert document embeddings to NumPy array\n",
    "document_embeddings = np.array(document_result)  # Shape: (79, 384)\n",
    "\n",
    "# Compute cosine similarity for each document\n",
    "similarities = cosine_similarity(query_result, document_embeddings)\n",
    "\n",
    "# Get the top 5 most similar documents\n",
    "top_indices = np.argsort(similarities)[::-1][:5]  # Sort in descending order and take top 5\n",
    "\n",
    "# Print results\n",
    "print(\"Top 5 Similar Documents:\")\n",
    "for i, idx in enumerate(top_indices):\n",
    "    print(f\"{i+1}. Document {idx} - Similarity: {similarities[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967da3a8-101f-4874-b5a3-8c723cd7a468",
   "metadata": {},
   "source": [
    "## intfloat/e5-small-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4a33556-6436-4fc9-9cbd-4f2031eb299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import tiktoken\n",
    "\n",
    "question = \"What is Pathophysiology?\"\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/e5-small-v2\")\n",
    "\n",
    "\n",
    "# Embed a single query\n",
    "query_result = embedding_model.embed_query(question)  \n",
    "\n",
    "# Embed multiple documents (convert to text first)\n",
    "document_result = embedding_model.embed_documents([doc.page_content for doc in docs])\n",
    "\n",
    "# Check the length of query embedding\n",
    "print(len(query_result))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34e128f-1b5c-4cae-a713-1a5f48e3e86c",
   "metadata": {},
   "source": [
    "# Cosine Similarity for intfloat/e5-small-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4693e143-ca06-44fd-ac1d-a33abfbadcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Similar Documents:\n",
      "1. Document 9 - Similarity: 0.8518\n",
      "2. Document 11 - Similarity: 0.8379\n",
      "3. Document 15 - Similarity: 0.8342\n",
      "4. Document 46 - Similarity: 0.8193\n",
      "5. Document 59 - Similarity: 0.8179\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec2, vec1)  # (79, 384) @ (384,) → (79,)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2, axis=1)  # Compute norms for each document\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Convert document embeddings to NumPy array\n",
    "document_embeddings = np.array(document_result)  # Shape: (79, 384)\n",
    "\n",
    "# Compute cosine similarity for each document\n",
    "similarities = cosine_similarity(query_result, document_embeddings)\n",
    "\n",
    "# Get the top 5 most similar documents\n",
    "top_indices = np.argsort(similarities)[::-1][:5]  # Sort in descending order and take top 5\n",
    "\n",
    "# Print results\n",
    "print(\"Top 5 Similar Documents:\")\n",
    "for i, idx in enumerate(top_indices):\n",
    "    print(f\"{i+1}. Document {idx} - Similarity: {similarities[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce46e2-4e42-45ef-8a9a-6b54ef6e102b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
